{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing..........\n",
      "Fetching details of match...........\n",
      "details fetched.............\n",
      "processing...........\n",
      "https://www.whoscored.com/Matches/1649738/Live/\n",
      "converting to csv file.............\n",
      "Finalizing........\n",
      "Please wait........\n",
      "Fetched data for  Events/01-14-Aug-22-ZulGen-event.csv\n",
      "Fetched data for  Players/14-Aug-22-ZulGen_playerid.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "\n",
    "urls=[]\n",
    "\n",
    "temp_url='https://www.whoscored.com/Matches/1649738/Live/'        \n",
    "urls.append(temp_url)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#for i in range(160,168):  \n",
    "\n",
    "\n",
    "   #  temp_url='https://www.whoscored.com/Matches/1546'+str(i)+'/Live/Netherlands-Eredivisie-2021-2022'\n",
    "\n",
    "    # urls.append(temp_url)\n",
    "\n",
    "    #temp_url='https://www.whoscored.com/Matches/1556'+str(i)+'/Live/England-League-One-2021-2022'\n",
    "    #urls.append(temp_url)\n",
    "\n",
    "\n",
    "for funt in range(len(urls)):\n",
    "\t\n",
    "    print('processing..........')\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(urls[funt])\n",
    "    time.sleep(2.4)\n",
    "    soup_level1=BeautifulSoup(driver.page_source, 'lxml')\n",
    "    #print(soup_level1.text)\n",
    "    soup_level=driver.page_source\n",
    "    driver.close()\n",
    "    print('Fetching details of match...........')\n",
    "    false='false'\n",
    "    true='true'\n",
    "    #text_file = open(\"sample.txt\",\"w\")\n",
    "    #n = text_file.write(soup_level1)\n",
    "    #text_file.close()\n",
    "    #r = requests.get(urls[funt])\n",
    "    #with open('file.txt','w') as file:\n",
    "    #    file.write(r.text)\n",
    "    krokuch=soup_level.split('{\"playerIdNameDictionary\":{')[1]\n",
    " \n",
    "    milakuch=krokuch.split('commonEvents')[1]\n",
    "    haukuch=milakuch.split('PostGame')\n",
    "    dddd=(haukuch[0]+'PostGame'+(haukuch[1])+'PostGame\"'+'}')\n",
    "    needed=dddd.split('{\"id')[1:]\n",
    "    # c1_id=[]\n",
    "    cleaned=[]\n",
    "    \n",
    "    for i in range(len(needed)-1):\n",
    "        cleaned.append(eval('{\"id'+needed[i][:-2]+'}'))\n",
    "    df=pd.DataFrame(cleaned)\n",
    "    print('details fetched.............')\n",
    "    #for column 'type'\n",
    "    types=list(df['type'])\n",
    "    types=pd.DataFrame(types)\n",
    "    typedisplayName='type/'+types.columns[0]\n",
    "    typevalue='type/'+types.columns[1]\n",
    "    types.rename(columns = {types.columns[0]:typedisplayName,types.columns[1]:typevalue}, inplace = True) \n",
    "    df=df.drop(columns=['type'])\n",
    "    df=pd.concat([df,types], axis=1)\n",
    "    #for column 'satisfiedEventsTypes'\n",
    "    satisfiedEventsTypess=list(df['satisfiedEventsTypes'])\n",
    "    satisfiedEventsTypess=pd.DataFrame(satisfiedEventsTypess)\n",
    "    s_temp=[]\n",
    "    for i in range(len(satisfiedEventsTypess.columns)):\n",
    "        s_temp.append('satisfiedEventsTypes/'+str(satisfiedEventsTypess.columns[i]))\n",
    "        satisfiedEventsTypess.rename(columns = {satisfiedEventsTypess.columns[i]:s_temp[-1]}, inplace = True)\n",
    "    df=df.drop(columns=['satisfiedEventsTypes'])\n",
    "    df=pd.concat([df,satisfiedEventsTypess], axis=1)\n",
    "    #for column 'period'\n",
    "    periods=list(df['period'])\n",
    "    periods=pd.DataFrame(periods)\n",
    "    s_temp=[]\n",
    "    for i in range(len(periods.columns)):\n",
    "        s_temp.append('period/'+str(periods.columns[i]))\n",
    "        periods.rename(columns = {periods.columns[i]:s_temp[-1]}, inplace = True)\n",
    "    df=df.drop(columns=['period'])\n",
    "    df=pd.concat([df,periods], axis=1)\n",
    "    #for column 'outcomeType'\n",
    "    outcomeTypes=list(df['outcomeType'])\n",
    "    outcomeTypes=pd.DataFrame(outcomeTypes)\n",
    "    s_temp=[]\n",
    "    for i in range(len(outcomeTypes.columns)):\n",
    "        s_temp.append('outcomeType/'+str(outcomeTypes.columns[i]))\n",
    "        outcomeTypes.rename(columns = {outcomeTypes.columns[i]:s_temp[-1]}, inplace = True)\n",
    "    df=df.drop(columns=['outcomeType'])\n",
    "    df=pd.concat([df,outcomeTypes], axis=1)\n",
    "    # cardTypess=list(df['cardType'])\n",
    "    # cardTypess=pd.DataFrame(cardTypes)\n",
    "    print('processing...........')\n",
    "    #for column 'qualifiers'\n",
    "    qualifierss=pd.DataFrame(list(df['qualifiers']))\n",
    "    print(urls[funt])\n",
    "    for i in range(len(qualifierss.columns)):\n",
    "        dfdf_1=pd.DataFrame(dict(qualifierss[i])).T\n",
    "        dfdf_type_1=pd.DataFrame(dict(dfdf_1['type'])).T#needed\n",
    "        s_temp=[]\n",
    "        for j in range(len(dfdf_type_1.columns)):\n",
    "            s_temp.append('qualifiers/'+str(i)+'/type/'+str(dfdf_type_1.columns[j]))\n",
    "            dfdf_type_1.rename(columns = {dfdf_type_1.columns[j]:s_temp[-1]}, inplace = True)\n",
    "        df=pd.concat([df,dfdf_type_1], axis=1)\n",
    "        dfdf_value_1=pd.DataFrame((pd.DataFrame(dict(qualifierss[0])).T)['value'])#needed\n",
    "        vald='qualifiers/'+str(i)+'/value'\n",
    "        dfdf_value_1.rename(columns = {'value':vald}, inplace = True)\n",
    "        df=pd.concat([df,dfdf_value_1], axis=1)\n",
    "    df=df.drop(columns=['qualifiers'])\n",
    "\n",
    "    print('converting to csv file.............')\n",
    "    #soup_level1 = soup_level2\n",
    "    date_match=soup_level1.find_all('div',{'class':'info-block cleared'})[2].text[-9:]\n",
    "    first_team=soup_level1.find_all('a',{'class':'team-link'})[0].text[:3]\n",
    "    first_team_home=soup_level1.find_all('a',{'class':'team-link'})[0].text\n",
    "    second_team=soup_level1.find_all('a',{'class':'team-link'})[1].text[:3]\n",
    "    second_team_away=soup_level1.find_all('a',{'class':'team-link'})[1].text\n",
    "    filename='01-'+date_match+'-'+first_team+second_team+'-event.csv'\n",
    "    \n",
    "    filename=filename.replace(\" \",'')\n",
    "    filename='Events/'+filename\n",
    "    col=df.columns.tolist()\n",
    "    try:\n",
    "        if(col[3]!='cardType'):\n",
    "            df.insert(3,'cardType',np.nan)\n",
    "            col=df.columns.tolist()\n",
    "    except:\n",
    "        print('Finalizing........')\n",
    "    try:\n",
    "        if(col[11]!='isGoal'):\n",
    "            df.insert(11,'isGoal',np.nan)\n",
    "            col=df.columns.tolist()\n",
    "    except:\n",
    "        print('Please wait........')\n",
    "    try:\n",
    "        if(col[1]!='Date'):\n",
    "            df.insert(1,'Date',date_match)\n",
    "            col=df.columns.tolist()\n",
    "    except:\n",
    "        print('Please wait date........')\n",
    "    try:\n",
    "        if(col[11]!='isOwnGoal'):\n",
    "            df.insert(11,'isOwnGoal',np.nan)\n",
    "            col=df.columns.tolist()\n",
    "    except:\n",
    "        print('Done.............')\n",
    "    try:\n",
    "        if(col[11]!='AwayTeam'):\n",
    "            df.insert(11,'AwayTeam',second_team_away)\n",
    "            col=df.columns.tolist()\n",
    "    except:\n",
    "        print('Home.............')\n",
    "    try:\n",
    "        if(col[11]!='HomeTeam'):\n",
    "            df.insert(11,'HomeTeam',first_team_home)\n",
    "            col=df.columns.tolist()\n",
    "    except:\n",
    "        print('Home.............')      \n",
    "\n",
    "\n",
    "    df.insert(0,'TeamId',df['teamId'])\n",
    "    df=df.drop(columns=['teamId'])\n",
    "    df.insert(1,'PlayerId',df['playerId'])\n",
    "    df=df.drop(columns=['playerId'])\n",
    "    \n",
    "    print('Fetched data for ',filename)\n",
    "\n",
    "    df.to_csv(filename,encoding='utf-8')\n",
    "\n",
    "    nishu=soup_level.split('{\"playerIdNameDictionary\":{')[1]\n",
    "    fir_data=nishu.split(',\"periodMinuteLimits')[0][:-1]\n",
    "    \n",
    "#     a=table[-13]\n",
    "#     fir_data=a.text.split(',\"periodMinuteLimits')\n",
    "#     tttt=fir_data[0]\n",
    "#     tttt=tttt[59:-1]\n",
    "    # fir_data=a.text[59:1116]\n",
    "    fir_data=fir_data.replace('\"','')\n",
    "    players=fir_data.split(',')\n",
    "    ids=[]\n",
    "    name=[]\n",
    "    #print(urls[fun])\n",
    "    for i in players:\n",
    "        id_name=i.split(':')\n",
    "        ids.append(id_name[0])\n",
    "        name.append(id_name[1])\n",
    "\n",
    "    date_match=soup_level1.find_all('div',{'class':'info-block cleared'})[2].text[-9:]\n",
    "    first_team=soup_level1.find_all('a',{'class':'team-link'})[0].text[:3]\n",
    "    second_team=soup_level1.find_all('a',{'class':'team-link'})[1].text[:3]\n",
    "    filename=date_match+'-'+first_team+second_team+'_playerid'+'.csv'\n",
    "    filename=filename.replace(\" \",'')\n",
    "    filename= 'Players/'+filename\n",
    "    print('Fetched data for ',filename)\n",
    "    fin=pd.DataFrame({'ID':ids,'Name':name})\n",
    "    fin.to_csv(filename,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.2.0-py3-none-any.whl (983 kB)\n",
      "\u001b[K     |████████████████████████████████| 983 kB 4.1 MB/s eta 0:00:01     |███████████████▋                | 481 kB 4.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting trio-websocket~=0.9\n",
      "  Using cached trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
      "Collecting trio~=0.17\n",
      "  Using cached trio-0.20.0-py3-none-any.whl (359 kB)\n",
      "Requirement already satisfied: urllib3[secure,socks]~=1.26 in ./.conda/envs/mypython3/lib/python3.8/site-packages (from selenium) (1.26.7)\n",
      "Requirement already satisfied: async-generator>=1.9 in ./.conda/envs/mypython3/lib/python3.8/site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: sniffio in ./.conda/envs/mypython3/lib/python3.8/site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: idna in ./.conda/envs/mypython3/lib/python3.8/site-packages (from trio~=0.17->selenium) (3.2)\n",
      "Requirement already satisfied: sortedcontainers in ./.conda/envs/mypython3/lib/python3.8/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in ./.conda/envs/mypython3/lib/python3.8/site-packages (from trio~=0.17->selenium) (21.2.0)\n",
      "Collecting outcome\n",
      "  Using cached outcome-1.1.0-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting wsproto>=0.14\n",
      "  Using cached wsproto-1.1.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14 in ./.conda/envs/mypython3/lib/python3.8/site-packages (from urllib3[secure,socks]~=1.26->selenium) (21.0.0)\n",
      "Requirement already satisfied: cryptography>=1.3.4 in ./.conda/envs/mypython3/lib/python3.8/site-packages (from urllib3[secure,socks]~=1.26->selenium) (3.4.8)\n",
      "Requirement already satisfied: certifi in ./.conda/envs/mypython3/lib/python3.8/site-packages (from urllib3[secure,socks]~=1.26->selenium) (2021.10.8)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in ./.conda/envs/mypython3/lib/python3.8/site-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: cffi>=1.12 in ./.conda/envs/mypython3/lib/python3.8/site-packages (from cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (1.14.6)\n",
      "Requirement already satisfied: pycparser in ./.conda/envs/mypython3/lib/python3.8/site-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (2.20)\n",
      "Requirement already satisfied: six>=1.5.2 in ./.conda/envs/mypython3/lib/python3.8/site-packages (from pyOpenSSL>=0.14->urllib3[secure,socks]~=1.26->selenium) (1.16.0)\n",
      "Collecting h11<1,>=0.9.0\n",
      "  Using cached h11-0.13.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: outcome, h11, wsproto, trio, trio-websocket, selenium\n",
      "Successfully installed h11-0.13.0 outcome-1.1.0 selenium-4.2.0 trio-0.20.0 trio-websocket-0.9.2 wsproto-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
